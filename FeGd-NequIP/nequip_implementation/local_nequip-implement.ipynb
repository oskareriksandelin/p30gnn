{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if they can be imported and their versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:41:03.635379Z",
     "iopub.status.busy": "2025-12-04T10:41:03.635076Z",
     "iopub.status.idle": "2025-12-04T10:41:03.686121Z",
     "shell.execute_reply": "2025-12-04T10:41:03.685304Z",
     "shell.execute_reply.started": "2025-12-04T10:41:03.635358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16.1\n",
      "2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import nequip\n",
    "import torch\n",
    "import os\n",
    "print(nequip.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\David\\OneDrive\\Documents\\GitHub\\p30gnn\\FeGd-NequIP\\nequip_implementation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy or upload the dataset to the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retracking and make simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the yaml file for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:47:37.997210Z",
     "iopub.status.busy": "2025-12-04T10:47:37.996838Z",
     "iopub.status.idle": "2025-12-04T10:47:38.122558Z",
     "shell.execute_reply": "2025-12-04T10:47:38.121515Z",
     "shell.execute_reply.started": "2025-12-04T10:47:37.997179Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sitraj.xyz  sitraj.xyz.1  training_network.yaml\n"
     ]
    }
   ],
   "source": [
    "training_network_yaml_string = \"\"\"\n",
    "\n",
    "\n",
    "# a simple example config file\n",
    "\n",
    "# Two folders will be used during the training: 'root'/process and 'root'/'run_name'\n",
    "# run_name contains logfiles and saved models\n",
    "# process contains processed data sets\n",
    "# if 'root'/'run_name' exists, 'root'/'run_name'_'year'-'month'-'day'-'hour'-'min'-'s' will be used instead.\n",
    "root: results/fegd\n",
    "run_name: fegd_training\n",
    "seed: 42                                                                         # model seed\n",
    "dataset_seed: 42                                                                 # data set seed\n",
    "append: true                                                                      # set true if a restarted run should append to the previous log file\n",
    "\n",
    "# see https://arxiv.org/abs/2304.10061 for discussion of numerical precision\n",
    "default_dtype: float32\n",
    "model_dtype: float32\n",
    "#allow_tf32: true    # consider setting to false if you plan to mix training/inference over any devices that are not NVIDIA Ampere or later\n",
    "\n",
    "# == network ==\n",
    "\n",
    "# `model_builders` defines a series of functions that will be called to construct the model\n",
    "# each model builder has the opportunity to update the model, the config, or both\n",
    "# model builders from other packages are allowed (see mir-group/allegro for an example); those from `nequip.model` don't require a prefix\n",
    "# these are the default model builders:\n",
    "model_builders:\n",
    " - SimpleIrrepsConfig         # update the config with all the irreps for the network if using the simplified `l_max` / `num_features` / `parity` syntax\n",
    " - EnergyModel                # build a full NequIP model\n",
    " - PerSpeciesRescale          # add per-atom / per-species scaling and shifting to the NequIP model before the total energy sum\n",
    " - StressForceOutput                # wrap the energy model in a module that uses autodifferention to compute the forces\n",
    " - RescaleEnergyEtc           # wrap the entire model in the appropriate global rescaling of the energy, forces, etc.\n",
    "#   ^ global rescaling blocks must always go last!\n",
    "r_max: 5.0                                                                        # cutoff radius in length units, here Angstrom, this is an important hyperparamter to scan\n",
    "num_layers: 2                                                                     # number of interaction blocks, we find 3-5 to work best\n",
    "l_max: 1                                                                          # the maximum irrep order (rotation order) for the network's features, l=2 is accurate but slower, l=1 if you want to be faster but less accurte\n",
    "parity: true                                                                      # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this\n",
    "num_features: 16                                                                  # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower\n",
    "nonlinearity_type: gate                                                           # may be 'gate' or 'norm', 'gate' is recommended\n",
    "\n",
    "# scalar nonlinearities to use — available options are silu, ssp (shifted softplus), tanh, and abs.\n",
    "# Different nonlinearities are specified for e (even) and o (odd) parity;\n",
    "# note that only tanh and abs are correct for o (odd parity)\n",
    "# silu typically works best for even \n",
    "nonlinearity_scalars:\n",
    "  e: silu\n",
    "  o: tanh\n",
    "\n",
    "nonlinearity_gates:\n",
    "  e: silu\n",
    "  o: tanh\n",
    "\n",
    "# radial network basis\n",
    "num_basis: 8                                                                      # number of basis functions used in the radial basis, 8 usually works best\n",
    "BesselBasis_trainable: true                                                       # set true to train the bessel weights\n",
    "PolynomialCutoff_p: 6                                                             # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance\n",
    "\n",
    "# radial network\n",
    "invariant_layers: 2                                                               # number of radial layers, usually 1-3 works best, smaller is faster\n",
    "invariant_neurons: 64                                                             # number of hidden neurons in radial function, smaller is faster\n",
    "avg_num_neighbors: auto                                                           # number of neighbors to divide by, null => no normalization, auto computes it based on dataset \n",
    "use_sc: true                                                                      # use self-connection or not, usually gives big improvement\n",
    "\n",
    "# data set\n",
    "# -- data --\n",
    "dataset: ase                                                                   \n",
    "dataset_file_name: fegd_dataset.xyz                      # path to data set file\n",
    "ase_args:\n",
    "  format: extxyz\n",
    "\n",
    "# A mapping of chemical species to type indexes is necessary if the dataset is provided with atomic numbers instead of type indexes.\n",
    "chemical_symbol_to_type:\n",
    "  Gd: 0\n",
    "  Fe: 1\n",
    "\n",
    "# logging\n",
    "wandb: false                                                                        # we recommend using wandb for logging\n",
    "wandb_project: FEGD                                                     # project name used in wandb\n",
    "\n",
    "verbose: info                                                                      # the same as python logging, e.g. warning, info, debug, error; case insensitive\n",
    "log_batch_freq: 100                                                                # batch frequency, how often to print training errors withinin the same epoch\n",
    "log_epoch_freq: 1                                                                  # epoch frequency, how often to print \n",
    "save_checkpoint_freq: -1                                                           # frequency to save the intermediate checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
    "save_ema_checkpoint_freq: -1                                                       # frequency to save the intermediate ema checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
    "\n",
    "# training\n",
    "n_train: 3206                                                                       # number of training data\n",
    "n_val: 401                                                                          # number of validation data\n",
    "learning_rate: 0.001                                                               # learning rate, we found values between 0.01 and 0.005 to work best - this is often one of the most important hyperparameters to tune\n",
    "batch_size: 4                                                                      # batch size, we found it important to keep this small for most applications including forces (1-5); for energy-only training, higher batch sizes work better\n",
    "validation_batch_size: 20                                                           # batch size for evaluating the model during validation. This does not affect the training results, but using the highest value possible (<=n_val) without running out of memory will speed up your training.\n",
    "max_epochs: 50                                                                 # stop training after _ number of epochs, we set a very large number, as e.g. 1million and then just use early stopping and not train the full number of epochs\n",
    "train_val_split: random                                                            # can be random or sequential. if sequential, first n_train elements are training, next n_val are val, else random, usually random is the right choice\n",
    "shuffle: true                                                                      # if true, the data loader will shuffle the data, usually a good idea\n",
    "metrics_key: validation_loss                                                       # metrics used for scheduling and saving best model. Options: `set`_`quantity`, set can be either \"train\" or \"validation, \"quantity\" can be loss or anything that appears in the validation batch step header, such as f_mae, f_rmse, e_mae, e_rmse\n",
    "use_ema: true                                                                      # if true, use exponential moving average on weights for val/test, usually helps a lot with training, in particular for energy errors\n",
    "ema_decay: 0.99                                                                    # ema weight, typically set to 0.99 or 0.999\n",
    "ema_use_num_updates: true                                                          # whether to use number of updates when computing averages\n",
    "report_init_validation: true                                                       # if True, report the validation error for just initialized model\n",
    "\n",
    "# early stopping based on metrics values.\n",
    "early_stopping_patiences:                                                          # stop early if a metric value stopped decreasing for n epochs\n",
    "  validation_loss: 50\n",
    "\n",
    "early_stopping_lower_bounds:                                                       # stop early if a metric value is lower than the bound\n",
    "  LR: 1.0e-5\n",
    "\n",
    "early_stopping_upper_bounds:                                                       # stop early if the training appears to have exploded\n",
    "  validation_loss: 1.0e+4\n",
    "\n",
    "# loss function\n",
    "loss_coeffs:                                                                        \n",
    "  total_energy:                                                                    \n",
    "    - 1\n",
    "    - PerAtomMSELoss\n",
    "\n",
    "# output metrics\n",
    "metrics_components:\n",
    "  - - total_energy\n",
    "    - mae    \n",
    "  - - total_energy\n",
    "    - mae\n",
    "    - PerAtom: True                        # if true, energy is normalized by the number of atoms\n",
    "\n",
    "# optimizer, may be any optimizer defined in torch.optim\n",
    "# the name `optimizer_name`is case sensitive\n",
    "# IMPORTANT: for NequIP (not for Allegro), we find that in most cases AMSGrad strongly improves\n",
    "# out-of-distribution generalization over Adam. We highly recommed trying both AMSGrad (by setting\n",
    "# optimizer_amsgrad: true) and Adam (by setting optimizer_amsgrad: false)\n",
    "optimizer_name: Adam                                                             \n",
    "optimizer_amsgrad: true\n",
    "\n",
    "# lr scheduler, currently only supports the two options listed in full.yaml, i.e. on-pleteau and cosine annealing with warm restarts, if you need more please file an issue\n",
    "# here: on-plateau, reduce lr by factory of lr_scheduler_factor if metrics_key hasn't improved for lr_scheduler_patience epoch\n",
    "lr_scheduler_name: ReduceLROnPlateau\n",
    "lr_scheduler_patience: 100\n",
    "lr_scheduler_factor: 0.5\n",
    "\n",
    "# we provide a series of options to shift and scale the data\n",
    "# these are for advanced use and usually the defaults work very well\n",
    "# the default is to scale the atomic energy and forces by scaling them by the force standard deviation and to shift the energy by the mean atomic energy\n",
    "# in certain cases, it can be useful to have a trainable shift/scale and to also have species-dependent shifts/scales for each atom\n",
    "\n",
    "# initial atomic energy shift for each species. default to the mean of per atom energy. Optional\n",
    "# the value can be a constant float value, an array for each species, or a string that defines a statistics over the training dataset\n",
    "# if numbers are explicitly provided, they must be in the same energy units as the training data\n",
    "per_species_rescale_shifts: dataset_per_atom_total_energy_mean\n",
    "\n",
    "# initial atomic energy scale for each species. Optional.\n",
    "# the value can be a constant float value, an array for each species, or a string\n",
    "# if numbers are explicitly provided, they must be in the same energy units as the training data\n",
    "per_species_rescale_scales: null\n",
    "\n",
    "\"\"\"\n",
    "with open(\"training_network.yaml\", \"w\") as f:\n",
    "    f.write(training_network_yaml_string)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:50:22.988247Z",
     "iopub.status.busy": "2025-12-04T10:50:22.987906Z",
     "iopub.status.idle": "2025-12-04T10:50:25.413258Z",
     "shell.execute_reply": "2025-12-04T10:50:25.412235Z",
     "shell.execute_reply.started": "2025-12-04T10:50:22.988222Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://kkb-production.jupyter-proxy.kaggle.net/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'https://kkb-production.jupyter-proxy.kaggle.net/'. Verify the server is running and reachable. (Invalid response: 404 Not Found).)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Get current working directory where training_network.yaml was created\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current working directory: {cwd}\")\n",
    "print(f\"Config file exists: {os.path.exists('training_network.yaml')}\")\n",
    "\n",
    "# Run training with Hydra using the current directory as config path\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"-m\", \"nequip.scripts.train\", \"--config-path\", cwd, \"--config-name\", \"training_network\"],\n",
    "    cwd=cwd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Test Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check training directory\n",
    "train_dir = \"results/fegd/fegd_training\"\n",
    "if os.path.exists(train_dir):\n",
    "    print(f\"✓ Training directory exists: {train_dir}\")\n",
    "    \n",
    "    # List contents\n",
    "    files = os.listdir(train_dir)\n",
    "    print(f\"\\nFiles in training directory ({len(files)} total):\")\n",
    "    for f in sorted(files):\n",
    "        fpath = os.path.join(train_dir, f)\n",
    "        size = os.path.getsize(fpath) if os.path.isfile(fpath) else \"dir\"\n",
    "        print(f\"  {f:<40} {size if isinstance(size, str) else f'{size/1024/1024:.1f}MB'}\")\n",
    "    \n",
    "    # Check for trainer.pth (checkpoint)\n",
    "    checkpoint = os.path.join(train_dir, \"trainer.pth\")\n",
    "    if os.path.exists(checkpoint):\n",
    "        size_mb = os.path.getsize(checkpoint) / 1024 / 1024\n",
    "        print(f\"\\n✓ Checkpoint found: trainer.pth ({size_mb:.1f}MB)\")\n",
    "    else:\n",
    "        print(\"\\n⏳ Checkpoint not yet created - training in progress\")\n",
    "    \n",
    "    # Check for log file\n",
    "    log_files = glob.glob(os.path.join(train_dir, \"*.log\"))\n",
    "    if log_files:\n",
    "        print(f\"\\n✓ Log files found:\")\n",
    "        for log in log_files:\n",
    "            with open(log, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            print(f\"  {os.path.basename(log)} ({len(lines)} lines)\")\n",
    "            # Show last few lines\n",
    "            if lines:\n",
    "                print(\"  Last lines:\")\n",
    "                for line in lines[-3:]:\n",
    "                    print(f\"    {line.strip()}\")\n",
    "else:\n",
    "    print(f\"✗ Training directory not found: {train_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the name of the directory that contains the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls results/fegd/\n",
    "!ls -lah results/fegd/fegd_training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nequip-evaluate --train-dir results/fegd/fegd_training --batch-size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nequip-deploy build --train-dir results/fegd/fegd_training fegd-deployed.pth\n",
    "!ls *pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the run and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n",
    "    \"\"\"\n",
    "    zip all the files in a directory\n",
    "    \n",
    "    Parameters\n",
    "    _____\n",
    "    directory: str\n",
    "        directory needs to be zipped, defualt is current working directory\n",
    "        \n",
    "    file_name: str\n",
    "        the name of the zipped file (including .zip), default is 'directory.zip'\n",
    "        \n",
    "    Returns\n",
    "    _____\n",
    "    Creates a hyperlink, which can be used to download the zip file)\n",
    "    \"\"\"\n",
    "    os.chdir(directory)\n",
    "    zip_ref = zipfile.ZipFile(file_name, mode='w')\n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file_name in file:\n",
    "                pass\n",
    "            else:\n",
    "                zip_ref.write(os.path.join(folder, file))\n",
    "\n",
    "    return FileLink(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "zip_dir('/kaggle/working/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8916173,
     "sourceId": 13988591,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nequip-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
